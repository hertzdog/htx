---
categories:
- Corso
date: 2025-06-03T03:29:10+0200
description: 'Abstract page for arXiv paper 2505.24864: ProRL: Prolonged Reinforcement
  Learning Expands Reasoning Boundaries in Large Language Models'
draft: false
feature_image: /images/posts/2025/09/prorl-prolonged-reinforcement-learning-large-language-models-featured.webp
images:
- /images/posts/2025/09/prorl-prolonged-reinforcement-learning-large-language-models-featured.webp
- /images/posts/2025/09/prorl-prolonged-reinforcement-learning-large-language-models-5.webp
language: en
last_linked: '2025-10-31T08:34:31.645583'
processed_date: 2025-09-06 10:48
related_articles:
- /posts/2025/05/2505-03335v2-absolute-zero-reinforced-self-play-re/
- /posts/2025/02/deepseek-r1-incentivizes-reasoning-in-llms-through/
- /posts/2025/06/the-illusion-of-thinking/
series:
- Articoli Interessanti
showDate: true
showPagination: true
showReadingTime: true
showWordCount: true
slug: 2505-24864-prorl-prolonged-reinforcement-learning
source_date: '2025-09-06'
source_type: Web Article
source_url: https://arxiv.org/abs/2505.24864
tags:
- LLM
- Foundation Model
title: '[2505.24864] ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries
  in Large Language Models'
---

{{< lead >}}
![Featured image](/images/posts/2025/09/prorl-prolonged-reinforcement-learning-large-language-models-featured.webp)
{{< /lead >}}

<small>
#### Fonte

**Tipo:** Web Article  
**Link originale:** [https://arxiv.org/abs/2505.24864](https://arxiv.org/abs/2505.24864)  
**Data pubblicazione:** 2025-09-06

</small>

---

## Sintesi

**WHAT** - ProRL è un metodo di addestramento che utilizza Reinforcement Learning prolungato per espandere le capacità di ragionamento dei modelli linguistici di grandi dimensioni. Questo approccio introduce tecniche come il controllo della divergenza KL, il reset della policy di riferimento e una varietà di compiti per migliorare le prestazioni di ragionamento.

**WHY** - ProRL è rilevante per il business AI perché dimostra che il RL prolungato può scoprire nuove strategie di ragionamento che non sono accessibili ai modelli base. Questo può portare a modelli linguistici più robusti e capaci di risolvere problemi complessi.

**WHO** - Gli autori principali sono Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz e Yi Dong. Il lavoro è stato pubblicato su arXiv, una piattaforma di preprint ampiamente utilizzata nella comunità scientifica.

**WHERE** - ProRL si posiziona nel mercato delle tecniche avanzate di addestramento per modelli linguistici, offrendo un'alternativa ai metodi tradizionali di addestramento.

**WHEN** - Il paper è stato pubblicato nel maggio 2025, indicando un approccio relativamente nuovo e innovativo nel campo del RL per modelli linguistici.

**BUSINESS IMPACT:**
- **Opportunità**: Implementare ProRL può migliorare significativamente le capacità di ragionamento dei nostri modelli linguistici, rendendoli più competitivi sul mercato.
- **Rischi**: La competizione con altre aziende che adottano tecniche simili potrebbe aumentare, richiedendo un continuo aggiornamento e innovazione.
- **Integrazione**: ProRL può essere integrato nello stack esistente di addestramento dei modelli linguistici, migliorando le prestazioni senza necessità di cambiamenti radicali.

**TECHNICAL SUMMARY:**
- **Core technology stack**: Utilizza tecniche di Reinforcement Learning, controllo della divergenza KL e reset della policy di riferimento.
- **Scalabilità e limiti architetturali**: ProRL richiede risorse computazionali significative per l'addestramento prolungato, ma offre miglioramenti sostanziali nelle capacità di ragionamento.
- **Differenziatori tecnici chiave**: L'uso di una varietà di compiti e il controllo della divergenza KL per scoprire nuove strategie di ragionamento.

---

## Casi d'uso

- **Private AI Stack**: Integrazione in pipeline proprietarie
- **Client Solutions**: Implementazione per progetti clienti
- **Strategic Intelligence**: Input per roadmap tecnologica
- **Competitive Analysis**: Monitoring ecosystem AI

---



## Risorse

### Link Originali
- [[2505.24864] ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models](https://arxiv.org/abs/2505.24864) - Link originale


---

*<small>Articolo segnalato e selezionato dal team Human Technology eXcellence elaborato tramite intelligenza artificiale (in questo caso con LLM HTX-EU-Mistral3.1Small) il 2025-09-06 10:48
Fonte originale: [https://arxiv.org/abs/2505.24864](https://arxiv.org/abs/2505.24864)</small>*

## Articoli Correlati

- [[2505.03335v2] Absolute Zero: Reinforced Self-play Reasoning with Zero Data](/posts/2025/05/2505-03335v2-absolute-zero-reinforced-self-play-re/) - *Tech*
- [DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning | Nature](/posts/2025/02/deepseek-r1-incentivizes-reasoning-in-llms-through/) - *LLM, AI, Best Practices*
- [The Illusion of Thinking](/posts/2025/06/the-illusion-of-thinking/) - *AI*